{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory is moved from /data2/sungjaecho/Projects/tacotron2/dev_ipynb to /data2/sungjaecho/Projects/tacotron2.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd_old = os.getcwd()\n",
    "os.chdir('..')\n",
    "cwd_new = os.getcwd()\n",
    "print(\"The working directory is moved from {} to {}.\".format(cwd_old, cwd_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing Gaussian Attention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Location Sensitive Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from layers import ConvNorm, LinearNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "                 attention_location_n_filters, attention_location_kernel_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
    "                                      bias=False, w_init_gain='tanh')\n",
    "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
    "                                       w_init_gain='tanh')\n",
    "        self.v = LinearNorm(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
    "                                            attention_location_kernel_size,\n",
    "                                            attention_dim)\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "\n",
    "    def get_alignment_energies(self, query, processed_memory,\n",
    "                               attention_weights_cat):\n",
    "        \"\"\"\n",
    "        PARAMS\n",
    "        ------\n",
    "        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
    "        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
    "        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        alignment (batch, max_time)\n",
    "        \"\"\"\n",
    "\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_attention_weights + processed_memory))\n",
    "\n",
    "        energies = energies.squeeze(-1)\n",
    "        return energies\n",
    "\n",
    "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
    "                attention_weights_cat, mask):\n",
    "        \"\"\"\n",
    "        PARAMS\n",
    "        ------\n",
    "        attention_hidden_state: attention rnn last output\n",
    "        memory: encoder outputs\n",
    "        processed_memory: processed encoder outputs\n",
    "        attention_weights_cat: previous and cummulative attention weights\n",
    "        mask: binary mask for padded data\n",
    "        \"\"\"\n",
    "        alignment = self.get_alignment_energies(\n",
    "            attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "        if mask is not None:\n",
    "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Gaussian Monotonic Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MontonicAttention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "                 attention_location_n_filters, attention_location_kernel_size):\n",
    "        super(MontonicAttention, self).__init__()\n",
    "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
    "                                      bias=False, w_init_gain='tanh')\n",
    "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
    "                                       w_init_gain='tanh')\n",
    "        self.mean_layer = LinearNorm(attention_dim, 10, bias=False, w_init_gain='sigmoid')\n",
    "        self.logvar_layer = LinearNorm(attention_dim, 1, bias=False, w_init_gain='linear')\n",
    "\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
    "                                            attention_location_kernel_size,\n",
    "                                            attention_dim)\n",
    "        self.score_mask_value = 0\n",
    "\n",
    "        self.prev_means = None\n",
    "        self.prev_vars = None\n",
    "\n",
    "\n",
    "    def normal_pdf(self, batch_txt_length, means, stds):\n",
    "        '''\n",
    "        PARAMS\n",
    "        -----\n",
    "        batch_txt_length: int.\n",
    "        means: torch.Tensor.\n",
    "        - size: [batch_size]\n",
    "        stds: torch.Tensor.\n",
    "        - size: [batch_size]\n",
    "\n",
    "        RETURNS\n",
    "        -----\n",
    "        p: torch.Tensor.\n",
    "        - size: [batch_size, batch_txt_length]\n",
    "        '''\n",
    "        enc_steps = batch_txt_length\n",
    "        batch_size = means.size(0)\n",
    "\n",
    "        means = means.unsqueeze(1).expand(means.size(0), enc_steps)\n",
    "        stds = stds.unsqueeze(1).expand(stds.size(0), enc_steps)\n",
    "\n",
    "        x = torch.Tensor(np.arange(enc_steps).reshape((1, enc_steps))).cuda()\n",
    "        x = x.expand(batch_size, enc_steps)\n",
    "\n",
    "        p = Normal(means, stds).cdf(x+0.5) - Normal(means, stds).cdf(x-0.5)\n",
    "        # p_sum is a normalizing factor to make the sum across the encoding dimension 1.\n",
    "        p_sum = p.sum(dim=1, keepdim=True).expand(p.size())\n",
    "        p = p / p_sum\n",
    "\n",
    "        return p\n",
    "\n",
    "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
    "                attention_weights_cat, mask):\n",
    "        \"\"\"\n",
    "        PARAMS\n",
    "        ------\n",
    "        attention_hidden_state: attention rnn last output (batch, n_mel_channels * n_frames_per_step)\n",
    "        memory: encoder outputs (B, T_in, attention_dim)\n",
    "        processed_memory: processed encoder outputs (B, T_in, encoder_embedding_dim)\n",
    "        attention_weights_cat: previous and cummulative attention weights (B, 2, max_time)\n",
    "        mask: binary mask for padded data\n",
    "        \"\"\"\n",
    "\n",
    "        processed_query = self.query_layer(attention_hidden_state.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        pred_features = torch.tanh(processed_query + processed_attention_weights + processed_memory)\n",
    "        # pred_features.size == (B, T_in, attention_dim)\n",
    "\n",
    "        # Average pooling across the second dimension\n",
    "        # avgpooled.size == (B, attention_dim)\n",
    "        avgpooled = pred_features.mean(dim=1)\n",
    "\n",
    "        # mean_increments.size == (B, 10)\n",
    "        mean_increments = F.sigmoid(self.mean_layer(avgpooled))\n",
    "        # mean_increment.size == (B)\n",
    "        mean_increment = mean_increments.sum(dim=-1)\n",
    "        if self.prev_means is None:\n",
    "            self.prev_means = torch.zeros_like(mean_increment).cuda()\n",
    "        means = self.prev_means + mean_increment\n",
    "        self.prev_means = means\n",
    "\n",
    "        # stds.size == (B)\n",
    "        variances = self.logvar_layer(avgpooled)\n",
    "        stds = variances.squeeze(-1).exp().sqrt()\n",
    "        self.prev_vars = variances\n",
    "\n",
    "        batch_txt_length = memory.size(1)\n",
    "        attention_weights = self.normal_pdf(batch_txt_length, means, stds)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_weights.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_steps = 10\n",
    "B = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.Tensor(np.arange(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = torch.Tensor(np.arange(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = means.unsqueeze(1).expand(means.size(0), enc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = stds.unsqueeze(1).expand(stds.size(0), enc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(np.arange(enc_steps).reshape((1, enc_steps)))\n",
    "x = x.expand(B, enc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10]), torch.Size([32, 10]), torch.Size([32, 10]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means.size(), stds.size(), x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Normal(means, stds).cdf(x+0.5) - Normal(means, stds).cdf(x-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sum = p.sum(dim=1, keepdim=True).expand(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p / p_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.Tensor(np.arange(B))\n",
    "stds = torch.Tensor(np.arange(B))\n",
    "batch_txt_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_steps = batch_txt_length\n",
    "batch_size = means.size(0)\n",
    "        \n",
    "means = means.unsqueeze(1).expand(means.size(0), enc_steps)\n",
    "stds = stds.unsqueeze(1).expand(stds.size(0), enc_steps)\n",
    "\n",
    "x = torch.Tensor(np.arange(enc_steps).reshape((1, enc_steps)))\n",
    "x = x.expand(batch_size, enc_steps)\n",
    "\n",
    "p = Normal(means, stds).cdf(x+0.5) - Normal(means, stds).cdf(x-0.5)\n",
    "# p_sum is a normalizing factor to make the sum across the encoding dimension 1.\n",
    "p_sum = p.sum(dim=1, keepdim=True).expand(p.size()) \n",
    "p = p / p_sum       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 10])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = torch.ones(batch_size, batch_txt_length, 512)\n",
    "memory.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 512])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(p.unsqueeze(1), memory).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(p.unsqueeze(1), memory).squeeze(1).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
