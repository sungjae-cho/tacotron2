{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory is moved from /data2/sungjaecho/Projects/tacotron2/dev_ipynb to /data2/sungjaecho/Projects/tacotron2.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd_old = os.getcwd()\n",
    "os.chdir('..')\n",
    "cwd_new = os.getcwd()\n",
    "print(\"The working directory is moved from {} to {}.\".format(cwd_old, cwd_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All measures for monotonicity of attention take `alignments`.\n",
    "\n",
    "`alignments`\n",
    "- type: `torch.Tensor`\n",
    "- size: `[batch_size, mel_steps, txt_steps]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have come up with 3 measures.\n",
    "- `forward_attention_ratio`: To measure monotonic increments over all two contiguous decoding steps.\n",
    "- `attention_ratio`: To measure how much encoding steps are attended over all encoding steps.\n",
    "- `multiple_attention_ratio`: To measure how much encoding steps are attended multiple times over all encoding steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_mel_length(gate_output):\n",
    "    '''\n",
    "    Prams\n",
    "    -----\n",
    "    gate_output: torch.Tensor.\n",
    "    - size: [max_mel_len].\n",
    "\n",
    "    Return\n",
    "    -----\n",
    "    mel_length: int.\n",
    "    - Size == [batch_size].\n",
    "    '''\n",
    "    #mel_length = torch.max(torch.argmax(alignments[batch_i,:,text_length-last_steps:text_length],dim=0))\n",
    "    #mel_length = mel_length.item()\n",
    "    is_positive_output = (gate_output > 0).tolist()\n",
    "    if True in is_positive_output:\n",
    "        mel_length = is_positive_output.index(True)\n",
    "    else:\n",
    "        mel_length = len(is_positive_output)\n",
    "\n",
    "    return mel_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. `forward_attention_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for this measure has already been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. `attention_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Development Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignments.size(): torch.Size([32, 80, 60])\n",
      "text_lengths.size(): torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size, mel_steps, txt_steps = 32, 80, 60\n",
    "alignments = torch.rand([batch_size, mel_steps, txt_steps])\n",
    "text_lengths = torch.randint(10, 60, [batch_size])\n",
    "mel_lengths = torch.randint(10, 80, [batch_size])\n",
    "print(\"alignments.size():\", alignments.size())\n",
    "print(\"text_lengths.size():\", text_lengths.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6319586038589478\n",
      "tensor([0.6744, 0.9697, 0.2549, 0.7692, 0.6250, 0.5800, 0.3913, 0.6957, 0.8947,\n",
      "        0.8333, 0.2909, 0.8684, 0.5000, 0.8222, 0.3137, 0.9091, 0.6429, 0.8500,\n",
      "        0.5536, 0.6364, 0.3111, 0.9545, 0.3333, 0.6000, 0.2414, 0.7736, 0.6857,\n",
      "        0.3889, 0.9259, 0.9118, 0.5862, 0.4348])\n"
     ]
    }
   ],
   "source": [
    "batch_size = alignments.size(0)\n",
    "batch_attention_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "for i in range(batch_size):\n",
    "    text_length = text_lengths[i].item()\n",
    "    mel_length = mel_lengths[i]\n",
    "    alignment = alignments[i,:mel_length,:text_length]\n",
    "    argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "    n_unique_argmax = torch.unique(argmax_alignment).size(0)\n",
    "    sample_attention_ratio = n_unique_argmax / text_length\n",
    "    batch_attention_ratio[i] = sample_attention_ratio\n",
    "mean_attention_ratio = batch_attention_ratio.mean().item()\n",
    "print(mean_attention_ratio)\n",
    "print(batch_attention_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Functionalization Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_ratio(alignments, text_lengths, gate_outputs):\n",
    "    '''\n",
    "    Attention ratio is a measure for \n",
    "    \"how much encoding steps are attended over all encoding steps\".\n",
    "    \n",
    "    Params\n",
    "    -----\n",
    "    alignments: Attention map. torch.Tensor. Shape: [batch_size, mel_steps, txt_steps].\n",
    "    text_lengths: torch.Tensor. A 1-D tensor that keeps input text lengths.\n",
    "    gate_outputs: torch.Tensor. Shape: [batch_size, stop_token_seq].\n",
    "    - A 2-D tensor that is a predicted sequence of the stopping decoding step\n",
    "    - 0 indicates a signal to generate the next decoding step.\n",
    "    - 1 indicates a signal to generate this decoding step and stop generating the next stop.\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    mean_attention_ratio\n",
    "    - float. torch.mean(batch_forward_attention_ratio).\n",
    "    batch_attention_ratio\n",
    "    - torch.Tensor((batch_size),dtype=torch.float).\n",
    "    '''\n",
    "    batch_size = alignments.size(0)\n",
    "    batch_attention_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "    sum_attention_ratio = 0\n",
    "    for i in range(batch_size):\n",
    "        text_length = text_lengths[i].item()\n",
    "        gate_output = gate_outputs[i]\n",
    "        mel_length = get_mel_length(gate_output)\n",
    "        alignment = alignments[i,:mel_length,:text_length]\n",
    "        argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "        n_unique_argmax = torch.unique(argmax_alignment).size(0)\n",
    "        sample_attention_ratio = n_unique_argmax / text_length\n",
    "        batch_attention_ratio[i] = sample_attention_ratio\n",
    "    mean_attention_ratio = batch_attention_ratio.mean().item()\n",
    "    \n",
    "    return mean_attention_ratio, batch_attention_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. `multiple_attention_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Development Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignments.size(): torch.Size([32, 80, 60])\n",
      "text_lengths.size(): torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size, mel_steps, txt_steps = 32, 80, 60\n",
    "alignments = torch.randint(0, 10, [batch_size, mel_steps, txt_steps])\n",
    "text_lengths = torch.randint(10, 60, [batch_size])\n",
    "mel_lengths = torch.randint(10, 80, [batch_size])\n",
    "print(\"alignments.size():\", alignments.size())\n",
    "print(\"text_lengths.size():\", text_lengths.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3225729167461395\n",
      "tensor([0.2727, 0.0339, 0.3571, 0.2105, 0.1020, 0.1667, 0.1944, 0.3846, 0.7692,\n",
      "        0.2586, 0.2037, 0.3438, 0.7778, 0.2889, 0.0513, 0.1087, 0.6522, 0.5000,\n",
      "        0.2083, 0.2917, 0.3000, 0.3243, 0.3611, 0.2449, 0.3077, 0.1034, 0.2745,\n",
      "        0.5000, 0.8824, 0.1750, 0.2857, 0.3871])\n"
     ]
    }
   ],
   "source": [
    "batch_size = alignments.size(0)\n",
    "batch_multiple_attention_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    text_length = text_lengths[i].item()\n",
    "    mel_length = mel_lengths[i].item()\n",
    "    alignment = alignments[i,:mel_length,:text_length]\n",
    "    argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "    argmax_alignment = argmax_alignment.tolist()\n",
    "    \n",
    "    for j in range((mel_length-2), -1, -1):\n",
    "        j_prev = j + 1\n",
    "        if argmax_alignment[j] == argmax_alignment[j_prev]:\n",
    "            del argmax_alignment[j_prev]\n",
    "        \n",
    "    n_multiple_attention = 0\n",
    "    for argmax in set(argmax_alignment):\n",
    "        if argmax_alignment.count(argmax) > 1:\n",
    "            n_multiple_attention += 1\n",
    "    sample_multiple_attention_ratio = n_multiple_attention / text_length\n",
    "    batch_multiple_attention_ratio[i] = sample_multiple_attention_ratio\n",
    "\n",
    "mean_multiple_attention_ratio = batch_multiple_attention_ratio.mean().item()\n",
    "print(mean_multiple_attention_ratio)\n",
    "print(batch_multiple_attention_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Functionalization Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_attention_ratio(alignments, text_lengths, gate_outputs):\n",
    "    '''\n",
    "    Multiple attention ratio is a measure for \n",
    "    \"how much encoding steps are attended multiple times over all encoding steps\".\n",
    "    \n",
    "    Params\n",
    "    -----\n",
    "    alignments: Attention map. torch.Tensor. Shape: [batch_size, mel_steps, txt_steps].\n",
    "    text_lengths: torch.Tensor. A 1-D tensor that keeps input text lengths.\n",
    "    gate_outputs: torch.Tensor. Shape: [batch_size, stop_token_seq].\n",
    "    - A 2-D tensor that is a predicted sequence of the stopping decoding step\n",
    "    - 0 indicates a signal to generate the next decoding step.\n",
    "    - 1 indicates a signal to generate this decoding step and stop generating the next stop.\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    mean_multiple_attention_ratio\n",
    "    - float. torch.mean(batch_forward_attention_ratio).\n",
    "    batch_multiple_attention_ratio\n",
    "    - torch.Tensor((batch_size),dtype=torch.float).\n",
    "    '''\n",
    "    batch_size = alignments.size(0)\n",
    "    batch_multiple_attention_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        text_length = text_lengths[i].item()\n",
    "        gate_output = gate_outputs[i]\n",
    "        mel_length = get_mel_length(gate_output)\n",
    "        alignment = alignments[i,:mel_length,:text_length]\n",
    "        argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "        argmax_alignment = argmax_alignment.tolist()\n",
    "\n",
    "        for j in range((mel_length-2), -1, -1):\n",
    "            j_prev = j + 1\n",
    "            if argmax_alignment[j] == argmax_alignment[j_prev]:\n",
    "                del argmax_alignment[j_prev]\n",
    "\n",
    "        n_multiple_attention = 0\n",
    "        for argmax in set(argmax_alignment):\n",
    "            if argmax_alignment.count(argmax) > 1:\n",
    "                n_multiple_attention += 1\n",
    "        sample_multiple_attention_ratio = n_multiple_attention / text_length\n",
    "        batch_multiple_attention_ratio[i] = sample_multiple_attention_ratio\n",
    "\n",
    "    mean_multiple_attention_ratio = batch_multiple_attention_ratio.mean().item()\n",
    "    \n",
    "    return mean_multiple_attention_ratio, batch_multiple_attention_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. `attention_range_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Development Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alignments.size(): torch.Size([32, 80, 60])\n",
      "text_lengths.size(): torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "batch_size, mel_steps, txt_steps = 32, 80, 60\n",
    "alignments = torch.rand([batch_size, mel_steps, txt_steps])\n",
    "text_lengths = torch.randint(10, 60, [batch_size])\n",
    "mel_lengths = torch.randint(10, 80, [batch_size])\n",
    "print(\"alignments.size():\", alignments.size())\n",
    "print(\"text_lengths.size():\", text_lengths.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9682873487472534\n",
      "tensor([1.0000, 1.0000, 0.7647, 1.0000, 0.9750, 1.0000, 1.0000, 0.9565, 1.0000,\n",
      "        1.0000, 0.9818, 1.0000, 0.9800, 0.9778, 0.9412, 1.0000, 0.9286, 0.9500,\n",
      "        1.0000, 0.9545, 1.0000, 1.0000, 0.9048, 0.8667, 0.8966, 1.0000, 0.9429,\n",
      "        0.9815, 1.0000, 1.0000, 0.9828, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "batch_size = alignments.size(0)\n",
    "batch_attention_range_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "for i in range(batch_size):\n",
    "    text_length = text_lengths[i].item()\n",
    "    mel_length = mel_lengths[i]\n",
    "    alignment = alignments[i,:mel_length,:text_length]\n",
    "    argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "    unique_argmax_set = torch.unique(argmax_alignment)\n",
    "    range_length = torch.max(unique_argmax_set) - torch.min(unique_argmax_set) + 1\n",
    "    range_length = range_length.item()\n",
    "    range_ratio = range_length / text_length\n",
    "    batch_attention_range_ratio[i] = range_ratio\n",
    "mean_attention_range_ratio = batch_attention_range_ratio.mean().item()\n",
    "print(mean_attention_range_ratio)\n",
    "print(batch_attention_range_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Functionalization Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_range_ratio(alignments, text_lengths, gate_outputs):\n",
    "    '''\n",
    "    Attention ratio is a measure for \n",
    "    \"how much encoding steps are attended over all encoding steps\".\n",
    "    \n",
    "    Params\n",
    "    -----\n",
    "    alignments: Attention map. torch.Tensor. Shape: [batch_size, mel_steps, txt_steps].\n",
    "    text_lengths: torch.Tensor. A 1-D tensor that keeps input text lengths.\n",
    "    gate_outputs: torch.Tensor. Shape: [batch_size, stop_token_seq].\n",
    "    - A 2-D tensor that is a predicted sequence of the stopping decoding step\n",
    "    - 0 indicates a signal to generate the next decoding step.\n",
    "    - 1 indicates a signal to generate this decoding step and stop generating the next stop.\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    mean_attention_ratio\n",
    "    - float. torch.mean(batch_forward_attention_ratio).\n",
    "    batch_attention_ratio\n",
    "    - torch.Tensor((batch_size),dtype=torch.float).\n",
    "    '''\n",
    "    batch_size = alignments.size(0)\n",
    "    batch_attention_range_ratio = torch.empty((batch_size), dtype=torch.float)\n",
    "    for i in range(batch_size):\n",
    "        text_length = text_lengths[i].item()\n",
    "        mel_length = mel_lengths[i]\n",
    "        alignment = alignments[i,:mel_length,:text_length]\n",
    "        argmax_alignment = torch.argmax(alignment, dim=1)\n",
    "        unique_argmax_set = torch.unique(argmax_alignment)\n",
    "        range_length = torch.max(unique_argmax_set) - torch.min(unique_argmax_set) + 1\n",
    "        range_length = range_length.item()\n",
    "        range_ratio = range_length / text_length\n",
    "        batch_attention_range_ratio[i] = range_ratio\n",
    "    mean_attention_range_ratio = batch_attention_range_ratio.mean().item()\n",
    "    \n",
    "    return mean_attention_range_ratio, batch_attention_range_ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
