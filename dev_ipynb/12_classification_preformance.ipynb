{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6,\n",
      " 'class 0': {'f1-score': 0.6666666666666666,\n",
      "             'precision': 0.5,\n",
      "             'recall': 1.0,\n",
      "             'support': 1},\n",
      " 'class 1': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 1},\n",
      " 'class 2': {'f1-score': 0.8,\n",
      "             'precision': 1.0,\n",
      "             'recall': 0.6666666666666666,\n",
      "             'support': 3},\n",
      " 'macro avg': {'f1-score': 0.48888888888888893,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.5555555555555555,\n",
      "               'support': 5},\n",
      " 'weighted avg': {'f1-score': 0.6133333333333334,\n",
      "                  'precision': 0.7,\n",
      "                  'recall': 0.6,\n",
      "                  'support': 5}}\n",
      "dict_keys(['class 0', 'class 1', 'class 2', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    }
   ],
   "source": [
    "report_dict = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6,\n",
      " 'class 0': {'f1-score': 0.6666666666666666,\n",
      "             'precision': 0.5,\n",
      "             'recall': 1.0,\n",
      "             'support': 1},\n",
      " 'class 1': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 1},\n",
      " 'class 2': {'f1-score': 0.8,\n",
      "             'precision': 1.0,\n",
      "             'recall': 0.6666666666666666,\n",
      "             'support': 3},\n",
      " 'macro avg': {'f1-score': 0.48888888888888893,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.5555555555555555,\n",
      "               'support': 5},\n",
      " 'weighted avg': {'f1-score': 0.6133333333333334,\n",
      "                  'precision': 0.7,\n",
      "                  'recall': 0.6,\n",
      "                  'support': 5}}\n"
     ]
    }
   ],
   "source": [
    "pprint(report_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['class 0', 'class 1', 'class 2', 'accuracy', 'macro avg', 'weighted avg'])\n"
     ]
    }
   ],
   "source": [
    "pprint(report_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}\n",
      "class 1 {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}\n",
      "class 2 {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}\n"
     ]
    }
   ],
   "source": [
    "for c in target_names:\n",
    "    print(c, report_dict[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = report_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class 0': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 1},\n",
       " 'class 1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
       " 'class 2': {'precision': 1.0,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.8,\n",
       "  'support': 3},\n",
       " 'accuracy': 0.6,\n",
       " 'macro avg': {'precision': 0.5,\n",
       "  'recall': 0.5555555555555555,\n",
       "  'f1-score': 0.48888888888888893,\n",
       "  'support': 5},\n",
       " 'weighted avg': {'precision': 0.7,\n",
       "  'recall': 0.6,\n",
       "  'f1-score': 0.6133333333333334,\n",
       "  'support': 5}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clsf_report(clsf_report1, clsf_report2):\n",
    "    '''\n",
    "    Add clsf_report2 to clsf_report1.\n",
    "    '''\n",
    "    for k in clsf_report1.keys():\n",
    "        if isinstance(clsf_report1[k], int) or isinstance(clsf_report1[k], float):\n",
    "            clsf_report1[k] += clsf_report2[k]\n",
    "        if isinstance(clsf_report1[k], dict):\n",
    "            for kk in clsf_report1[k].keys():\n",
    "                clsf_report1[k][kk] += clsf_report2[k][kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_clsf_report(d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clsf_report(clsf_report1, clsf_report2):\n",
    "    '''\n",
    "    Add clsf_report2 to clsf_report1.\n",
    "    clsf_report1 and clsf_report2 are out_dict of sklearn.metrics.classification_report.\n",
    "    '''\n",
    "    for k in clsf_report1.keys():\n",
    "        if isinstance(clsf_report1[k], int) or isinstance(clsf_report1[k], float):\n",
    "            clsf_report1[k] += clsf_report2[k]\n",
    "        if isinstance(clsf_report1[k], dict):\n",
    "            for kk in clsf_report1[k].keys():\n",
    "                clsf_report1[k][kk] += clsf_report2[k][kk]\n",
    "\n",
    "    return clsf_report1\n",
    "\n",
    "def divide_clsf_report(clsf_report, scalar):\n",
    "    '''\n",
    "    Divide all values in clsf_report by scalar.\n",
    "    clsf_report is out_dict of sklearn.metrics.classification_report.\n",
    "    '''\n",
    "    for k in clsf_report.keys():\n",
    "        if isinstance(clsf_report[k], int) or isinstance(clsf_report[k], float):\n",
    "            clsf_report[k] /= scalar\n",
    "        if isinstance(clsf_report[k], dict):\n",
    "            for kk in clsf_report[k].keys():\n",
    "                clsf_report[k][kk] /= scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve the following error: \n",
    "\n",
    "ValueError: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 1, 2, 2, 3]\n",
    "y_pred = [0, 0, 2, 2, 3]\n",
    "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3c5f5cc7a4f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict)\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1876\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1877\u001b[0m             )\n\u001b[1;32m   1878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 5. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "report_dict = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class 0': {'f1-score': 0.6666666666666666,\n",
      "             'precision': 0.5,\n",
      "             'recall': 1.0,\n",
      "             'support': 1},\n",
      " 'class 1': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 1},\n",
      " 'class 2': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 2},\n",
      " 'class 3': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1},\n",
      " 'class 4': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
      " 'macro avg': {'f1-score': 0.5333333333333333,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.6,\n",
      "               'support': 5},\n",
      " 'micro avg': {'f1-score': 0.8000000000000002,\n",
      "               'precision': 0.8,\n",
      "               'recall': 0.8,\n",
      "               'support': 5},\n",
      " 'weighted avg': {'f1-score': 0.7333333333333333,\n",
      "                  'precision': 0.7,\n",
      "                  'recall': 0.8,\n",
      "                  'support': 5}}\n"
     ]
    }
   ],
   "source": [
    "report_dict = classification_report(y_true, y_pred, labels=list(range(len(target_names))), target_names=target_names, output_dict=True)\n",
    "pprint(report_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
